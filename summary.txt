朴素贝叶斯：直接通过X和Y之前的概率关系来推测新数据的类别。这是一种比较土比较简单的关系，因为假设是自变量是线性无关的。对于很多问题，这个假设都不一定成立。
线性回归：先求cost函数，然后通过梯度下降，求参数，目标是cost最小。相对于朴素贝叶斯，线性回归可以解决自变量相关的问题。
逻辑回归：在线性规划的基础上，把Y值的范围映射到了[0,1]这个空间，依然是求cost函数，利用梯度下降最优化。
SVM：在逻辑回归的基础上，加上限制条件，将目标从“点到线或者平面距离最小”变为找到一个分割平面，使得这个平面到类的margin的距离最大，相当于使得不同类之间的距离尽量大。
神经网络：每一个节点就是一个逻辑回归函数，需要前向传播来求cost（因为中间层没有实际的Y值，只从从后到前来推），进而还是梯度下降最优化。
决策树：这个也应该算是土办法，好处是没有参数，坏处是容易过拟合，而且不能做online learning。前面的不少方法都可以通过随机梯度下降来实现online learning。但是决策树则正是由于没有参数，每次分割点是利用所有的数据计算的，于是加了一个数字，分割点都变了，就不能online learning了。random forest和boosting则是为了解决这些问题而提出的。
